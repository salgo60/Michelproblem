{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f1fd6c",
   "metadata": {},
   "source": [
    "Kod https://huggingface.co/facebook/vit-mae-base\n",
    "\n",
    "* [denna Notebook](https://github.com/salgo60/Michelproblem/blob/main/Notebook/Vision%20Transformer%20(base-sized%20model)%20pre-trained%20with%20MAE.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea108421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run:  2023-12-03 10:49:25.748721\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start_time  = datetime.now()\n",
    "print(\"Last run: \", start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a766dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, ViTMAEForPreTraining\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/vit-mae-base')\n",
    "model = ViTMAEForPreTraining.from_pretrained('facebook/vit-mae-base')\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "loss_vit_mae_base = outputs.loss\n",
    "mask = outputs.mask\n",
    "ids_restore = outputs.ids_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0a2c39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ended:  2023-12-03 10:49:33.289022\n",
      "Time elapsed vit-mae-base (hh:mm:ss.ms) 0:00:07.541169\n"
     ]
    }
   ],
   "source": [
    "end_time_vit_mae_base = datetime.now()\n",
    "print(\"Ended: \", end_time_vit_mae_base)\n",
    "time_vit_mae_base = datetime.now() - start_time\n",
    "print('Time elapsed vit-mae-base (hh:mm:ss.ms) {}'.format(time_vit_mae_base))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f11d1194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[158,  33,  49,  86,  98, 168, 180,  74, 171,  90, 169,  81,  70, 193,\n",
       "         147,  68, 128,  31,  22, 119,  16, 113, 114, 123, 126,  37,   5, 194,\n",
       "           2, 155, 112, 195, 183,  55,  45, 142, 144, 167, 105,  34,  97, 130,\n",
       "          94, 140, 163,   9,  50,  44,  21, 139,  69, 182,  19, 134,  18,  62,\n",
       "         190,   8,  89, 135,  48, 138, 148, 165, 117,  73, 110, 178,  36, 141,\n",
       "          10, 149, 124, 188, 172,  28,  54, 192,  13,  78, 143,  85,  27,  84,\n",
       "         101, 181, 121,  83, 189,  43, 120, 184,  26,  88,  29,   3, 173,  46,\n",
       "         146, 156,  15,  57, 132,  60,  17, 131,  25, 103,  14,  63, 118,  32,\n",
       "         137,  82, 115, 109,  87,   4, 108, 100,  11, 136, 164, 160,  47,  52,\n",
       "          24, 161, 179, 107, 122, 129, 185,  67, 170,  56, 159, 175,  91,  79,\n",
       "           7, 111,   6, 187, 116,  76,  72,  59,  35, 186,  30,  20,  80,  61,\n",
       "         127, 106,  51,  58, 174, 176,  99,  66, 104, 157,  65, 102, 145, 153,\n",
       "         154, 151,   1,  53,   0,  38, 125,  64,  12,  92, 166, 133,  41, 150,\n",
       "         191, 162,  71,  40,  95,  93,  75,  39,  77, 152,  96,  23, 177,  42]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45b526ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
       "         0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
       "         0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
       "         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
       "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96f39226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3438, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_vit_mae_base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c74b806",
   "metadata": {},
   "source": [
    "### facebook/vit-mae-huge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b4b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, ViTMAEForPreTraining\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/vit-mae-huge')\n",
    "model = ViTMAEForPreTraining.from_pretrained('facebook/vit-mae-huge')\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "loss_vit_mae_huge  = outputs.loss\n",
    "mask = outputs.mask\n",
    "ids_restore = outputs.ids_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96943c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3010, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_vit_mae_huge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ae387de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ended:  2023-12-03 10:49:48.760313\n",
      "Time elapsed vit-mae-base (hh:mm:ss.ms) 0:00:07.541169\n",
      "Time elapsed vit-mae-huge (hh:mm:ss.ms) 0:00:15.471974\n",
      "\n",
      "Time elapsed both (hh:mm:ss.ms) 0:00:23.012357\n"
     ]
    }
   ],
   "source": [
    "end_time_vit_mae_huge  = datetime.now()\n",
    "print(\"Ended: \", end_time_vit_mae_huge)\n",
    "print('Time elapsed vit-mae-base (hh:mm:ss.ms) {}'.format(time_vit_mae_base))\n",
    "print('Time elapsed vit-mae-huge (hh:mm:ss.ms) {}'.format(datetime.now() - end_time_vit_mae_base))\n",
    "\n",
    "print('\\nTime elapsed both (hh:mm:ss.ms) {}'.format(datetime.now() - start_time))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
